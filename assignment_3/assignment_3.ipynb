{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEfEJVmSXS_h"
   },
   "source": [
    "# Assignment-3 COMP-5630 Jacob Murrah\n",
    "## README\n",
    "This notebook implements natural language processing tasks focusing on word embeddings, n-gram classification, and sequence-to-sequence models for text summarization. It explores different embedding techniques, evaluates n-gram features for classification, and implements RNN-based models for summarizing legislative text.\n",
    "\n",
    "## Dependencies\n",
    "- **Python 3.x**\n",
    "- **matplotlib**\n",
    "- **seaborn**\n",
    "- **nltk**\n",
    "- **numpy==1.26.4**\n",
    "- **torch==2.2.2**\n",
    "- **transformers==4.50.3**\n",
    "- **datasets==2.18.0**\n",
    "- **scipy==1.12.0**\n",
    "- **pandas==2.1.4**\n",
    "- **scikit-learn==1.4.0**\n",
    "- **gensim==4.3.2**\n",
    "- **evaluate==0.4.1**\n",
    "\n",
    "## Instructions\n",
    "1. Run the cell with the skip cell magic function defined. Marked with the # SKIP MAGIC FUNCTION comment.\n",
    "2. Run the cell beginning with %%skip and marked with the # DOWNLOAD PACKAGES CELL comment.\n",
    "3. After downloads are complete the runtime will be disconnected and you will see an associated message.\n",
    "4. Reconnect the runtime and click on \\\"Runtime\\\" > \\\"Run all\\\" to execute the entire notebook sequentially.\n",
    "6. The notebook is organized into several sections. Ensure that all cells run without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlReYxVjagE9"
   },
   "outputs": [],
   "source": [
    "SKIP_INSTALL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emr_zPDbDzTV"
   },
   "outputs": [],
   "source": [
    "# SKIP MAGIC FUNCTION\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "  if globals().get('SKIP_INSTALL', False):\n",
    "    print(\"Skipping installation cell.\")\n",
    "  else:\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yj6sNu9vcAt6"
   },
   "outputs": [],
   "source": [
    "# SKIP_INSTALL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9A3z2JkJ2R7",
    "outputId": "dcf5fb54-88f2-4868-9d84-4c2889d5d300"
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# DOWNLOAD PACKAGES CELL\n",
    "import contextlib\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "\n",
    "def capture_output():\n",
    "  return contextlib.redirect_stdout(io.StringIO())\n",
    "\n",
    "print(\"Downloads take ~5 minutes but are required for version compatiability!\")\n",
    "time.sleep(1)\n",
    "\n",
    "with capture_output():\n",
    "  package_versions = {\n",
    "    \"numpy\": \"1.26.4\",\n",
    "    \"torch\": \"2.2.2\",\n",
    "    \"transformers\": \"4.50.3\",\n",
    "    \"datasets\": \"2.18.0\",\n",
    "    \"scipy\": \"1.12.0\",\n",
    "    \"pandas\": \"2.1.4\",\n",
    "    \"scikit-learn\": \"1.4.0\",\n",
    "    \"gensim\": \"4.3.2\",\n",
    "    \"evaluate\": \"0.4.1\"\n",
    "  }\n",
    "\n",
    "  # get cuda version\n",
    "  cuda_version_str = \"N/A\"\n",
    "  cuda_major_minor = None\n",
    "  nvcc_output = subprocess.check_output(['nvcc', '--version']).decode('utf-8')\n",
    "  match_obj = re.search(r'release (\\d+\\.\\d+)', nvcc_output)\n",
    "  if match_obj:\n",
    "    cuda_version_str = match_obj.group(1)\n",
    "    cuda_major_minor = \"\".join(cuda_version_str.split('.'))\n",
    "\n",
    "  # pytorch installation\n",
    "  pytorch_version = package_versions[\"torch\"]\n",
    "  pytorch_cuda_suffix = \"cu118\"\n",
    "  if cuda_major_minor and int(cuda_major_minor) >= 121:\n",
    "    pytorch_cuda_suffix = \"cu121\"\n",
    "\n",
    "  torchvision_version = \"0.17.2\"\n",
    "  torchaudio_version = \"2.2.2\"\n",
    "  pytorch_index_url = f\"https://download.pytorch.org/whl/{pytorch_cuda_suffix}\"\n",
    "\n",
    "  pytorch_install_command = (\n",
    "    f\"pip3 install \"\n",
    "    f\"torch=={pytorch_version}+{pytorch_cuda_suffix} \"\n",
    "    f\"torchvision=={torchvision_version}+{pytorch_cuda_suffix} \"\n",
    "    f\"torchaudio=={torchaudio_version}+{pytorch_cuda_suffix} \"\n",
    "    f\"--index-url {pytorch_index_url} --upgrade --no-cache-dir\"\n",
    "  )\n",
    "  get_ipython().system(pytorch_install_command)\n",
    "\n",
    "  # install specified packages\n",
    "  get_ipython().system('pip3 uninstall transformers -y')\n",
    "  packages_to_install = [\n",
    "    f\"{pkg}=={ver}\" for pkg, ver in package_versions.items() if pkg != \"torch\"\n",
    "  ]\n",
    "  other_packages_install_command = (\n",
    "    \"pip3 install --upgrade --no-cache-dir \" + \" \".join(packages_to_install)\n",
    "  )\n",
    "\n",
    "  get_ipython().system(other_packages_install_command)\n",
    "  get_ipython().system(\n",
    "    'pip install \"huggingface_hub[hf_xet]\" --upgrade --no-cache-dir'\n",
    "  )\n",
    "\n",
    "# restart environment\n",
    "print(\"Package installation completed successfully!\")\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crWLttL5M5VI",
    "outputId": "b7963d64-eb49-4149-bc74-345e3bae0541"
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "import importlib.metadata\n",
    "import sys\n",
    "version = importlib.metadata.version(\"torch\")\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE5wn2vIqw0Z"
   },
   "source": [
    "# Part 1. Word Embeddings and N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lr6cxAY9X18x"
   },
   "outputs": [],
   "source": [
    "## imports for word embedding\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6EywEfqWt0K"
   },
   "source": [
    "## Part 1. (1a-1b) You will examine two-word embeddings. You are given the following words:\n",
    "\n",
    "Dog, Bark, Tree, Bank, River, Money.\n",
    "\n",
    "Do the following:\n",
    "- Use Glove-twitter-50D word2vec and compute nxn matrices using cosine similarities for the given words.\n",
    "- Use FastText and compute nxn matrices using cosine similarities for the given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsoKj7EaDwhF",
    "outputId": "699521bf-bf03-40f6-aa31-9bcd6c7f4566"
   },
   "outputs": [],
   "source": [
    "# download glove-twitter-50 model\n",
    "nltk.download(\"twitter_samples\")\n",
    "nltk.download('stopwords')\n",
    "if \"glove\" not in globals():\n",
    "  glove = api.load(\"glove-twitter-50\")\n",
    "\n",
    "print(\"downloads completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9i94n6xsc6eW",
    "outputId": "63632a67-86f0-4728-b44f-afc60a367f98"
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "# https://www.kaggle.com/code/piyushagni5/sentiment-analysis-on-twitter-dataset-nlp\n",
    "def process_tweet(tweet):\n",
    "  stemmer = PorterStemmer()\n",
    "  stopwords_english = stopwords.words('english')\n",
    "\n",
    "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "  tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "  tokenizer = TweetTokenizer(\n",
    "    preserve_case=False, strip_handles=True, reduce_len=True\n",
    "  )\n",
    "  tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "  tweets_clean = []\n",
    "  for word in tweet_tokens:\n",
    "    if (word not in stopwords_english and word not in string.punctuation):\n",
    "      stem_word = stemmer.stem(word)\n",
    "      tweets_clean.append(stem_word)\n",
    "\n",
    "  return tweets_clean\n",
    "\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "common_texts = [\n",
    "  process_tweet(tweet)\n",
    "  for tweet_list in [negative_tweets, positive_tweets]\n",
    "  for tweet in tweet_list\n",
    "]\n",
    "print(f\"Number of tweets: {len(common_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xrAbDUoNRKI",
    "outputId": "32dd36fa-932d-4990-cc61-d9686bcd0b8c"
   },
   "outputs": [],
   "source": [
    "# train the Word2Vec and FastText models\n",
    "# NOTE: our GloVe w2v model is already loaded\n",
    "if \"w2v\" not in globals():\n",
    "  w2v = Word2Vec(sentences=common_texts, vector_size=50, window=5, min_count=1)\n",
    "if \"ft\" not in globals():\n",
    "  ft = FastText(\n",
    "    vector_size=50, window=5, min_count=1, sentences=common_texts, epochs=10\n",
    "  )\n",
    "print(\"done training models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O07OA-G4ZNh5"
   },
   "outputs": [],
   "source": [
    "# create similarity matrices of the words for each model\n",
    "def create_similarity_matrix(model, words, glove_model=False):\n",
    "  if glove_model:\n",
    "    common_text_words = set([word for text in common_texts for word in text])\n",
    "    glove_words = [\n",
    "      word.lower() for word in glove.key_to_index\n",
    "      if word.lower() in common_text_words\n",
    "    ]\n",
    "    words_in_model = [w for w in words if w in glove_words]\n",
    "  else:\n",
    "    words_in_model = [w for w in words if w in model.wv]\n",
    "\n",
    "  word_vectors = np.array(\n",
    "    [\n",
    "      model.get_vector(word) if glove_model else model.wv[word]\n",
    "      for word in words_in_model\n",
    "    ]\n",
    "  )\n",
    "  similarity_matrix = cosine_similarity(word_vectors)\n",
    "  return similarity_matrix, words_in_model\n",
    "\n",
    "words = [\"dog\", \"bark\", \"tree\", \"bank\", \"river\", \"money\"]\n",
    "glove_matrix, words_in_glove = create_similarity_matrix(\n",
    "  glove, words, glove_model=True\n",
    ")\n",
    "w2v_matrix, words_in_w2v = create_similarity_matrix(w2v, words)\n",
    "ft_matrix, words_in_ft = create_similarity_matrix(ft, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "VCiS0O2fAB6i",
    "outputId": "d7f91ce5-b002-434c-e0f7-fbf77be1e428"
   },
   "outputs": [],
   "source": [
    "# plot the similarity matrices for each model\n",
    "def plot_similarity_matrices(matrices, words_lists, titles):\n",
    "  n = len(matrices)\n",
    "  fig, axes = plt.subplots(1, n, figsize=(n * 6, 5))\n",
    "  for i in range(n):\n",
    "    sns.heatmap(\n",
    "      matrices[i],\n",
    "      annot=True,\n",
    "      fmt=\".2f\",\n",
    "      cmap=\"YlGnBu\",\n",
    "      xticklabels=words_lists[i],\n",
    "      yticklabels=words_lists[i],\n",
    "      ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(titles[i], fontsize=14)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "matrices = [glove_matrix, w2v_matrix, ft_matrix]\n",
    "words_lists = [words_in_glove, words_in_w2v, words_in_ft]\n",
    "titles = [\"GloVe Similarity\", \"Word2Vec Similarity\", \"FastText Similarity\"]\n",
    "plot_similarity_matrices(matrices, words_lists, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8JCOqOqBtMx"
   },
   "source": [
    "## Part 1. (1c) Which embedding captures better semantics? Justify your answer.<br>\n",
    "**Answer:** Based on the similarity scores between the 6 words, the GloVe model is better at capturing the semantics, i.e the meaning, of the words. The FastText model is a close second. Since there is not a universal standard similarity score for these specific word pairs I am using my own understanding of the english language to evaluate.\n",
    "\n",
    "Words with multiple meanings:\n",
    "- Bark: Dog barking or tree bark\n",
    "- Bank: River bank or a bank that manages money\n",
    "\n",
    "Related pairs:\n",
    "- Dog-Bark\n",
    "- Tree-Bark\n",
    "- Money-Bank\n",
    "- River-Bank\n",
    "\n",
    "Model Evaluation:\n",
    "- **GloVe:** The similarity matrix shows clear differentiation between semantically distinct words. For example, the similarity between “bark” and “bank” is extremely low and the similarity between \"money\" and \"bank\" is relatively high.\n",
    "- **Word2Vec:** Shows high similarity scores across most of the pairs and does not differentiate unrelated words. For example, the words \"dog\" and \"tree\" are unrelated but have an extremely high similarity score.\n",
    "- **FastText:** Is able to capture semantic of words but at a more moderate extent than GloVe. This moderate scoring does not differentiate as clearly between related and unrelated pairs as GloVe does.\n",
    "\n",
    "Based only on their 6-word similarity matrices, the GloVe model provides more realistic similarity scores and thus captures the semantic similarities between words better than the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSAX_NSMEwsJ",
    "outputId": "f7d665fa-5fc0-4db2-9cbc-d2891346f972"
   },
   "outputs": [],
   "source": [
    "benchmark = \"simlex999.txt\"\n",
    "result_glove = glove.evaluate_word_pairs(datapath(benchmark))\n",
    "result_w2v = w2v.wv.evaluate_word_pairs(datapath(benchmark))\n",
    "result_ft = ft.wv.evaluate_word_pairs(datapath(benchmark))\n",
    "\n",
    "print(f\"GloVe Spearman Measurements:\")\n",
    "print(f\" - Coefficient: {result_glove[1][0]:.4f}\")\n",
    "print(f\" - P-Value: {result_glove[1][1]:.4f}\")\n",
    "\n",
    "print(f\"Word2Vec Spearman Measurements:\")\n",
    "print(f\" - Coefficient: {result_w2v[1][0]:.4f}\")\n",
    "print(f\" - P-Value: {result_w2v[1][1]:.4f}\")\n",
    "\n",
    "print(f\"FastText Spearman Measurements:\")\n",
    "print(f\" - Coefficient: {result_ft[1][0]:.4f}\")\n",
    "print(f\" - P-Value: {result_ft[1][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcIbTVPtT7YI"
   },
   "source": [
    "## Part 1. (1c) Continued analysis...<br>\n",
    "**Answer:** I wanted to see which model performed better on a larger set of words so I measured the Spearman Coefficients of each model on the 'simlex999' dataset. In our context, the Spearman Coefficient judges how well our model's predictions match a human's analysis of word pairs. A higher coefficient shows a better match. On the 'simlex999' dataset, the GloVe model achieves the highest Spearman coefficient and is the only model with a statistically significant p-value, indicating that it does the best at capturing overall semantic relationships across a broader range of word pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBEXJI1jYdWZ"
   },
   "source": [
    "## Part 1. (2a-2c) N-grams and Classification\n",
    "\n",
    "Do the following:\n",
    "- Split the data 70% training and 30% testing.\n",
    "- Extract n-grams for n in [1, 4]. unigram, bigram, trigram, 4-grams.\n",
    "- Build a logistic regression model using n-gram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xECkSwbOaNnm"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjHz3_gEZ8_E"
   },
   "outputs": [],
   "source": [
    "# split the data into 70% training and 30% testing\n",
    "seed = 2025\n",
    "random.seed(seed)\n",
    "labels = np.array([0] * len(negative_tweets) + [1] * len(positive_tweets))\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "  common_texts, labels, test_size=0.3, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBHZT0Rra0hM"
   },
   "outputs": [],
   "source": [
    "# extract n-grams for n in [1, 4]. unigram, bigram, trigram, 4grams.\n",
    "def get_text_ngrams(text, n_range):\n",
    "  text_ngrams = []\n",
    "  for i in n_range:\n",
    "    text_string = [\"_\".join(gram) for gram in list(ngrams(text, i))]\n",
    "    text_ngrams.extend(text_string)\n",
    "\n",
    "  return \" \".join(text_ngrams)\n",
    "\n",
    "train_features = [get_text_ngrams(text, range(1, 5)) for text in train_texts]\n",
    "test_features = [get_text_ngrams(text, range(1, 5)) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeUFzjSSx6cG",
    "outputId": "46acb73d-2b93-4de5-b145-95ec436bb0b6"
   },
   "outputs": [],
   "source": [
    "# logistic regression model\n",
    "def train_and_predict_model(\n",
    "  train_features, test_features, train_labels, test_labels\n",
    "):\n",
    "  vectorizer = CountVectorizer()\n",
    "  X_train = vectorizer.fit_transform(train_features)\n",
    "  X_test = vectorizer.transform(test_features)\n",
    "\n",
    "  model = LogisticRegression(max_iter=1000)\n",
    "  model.fit(X_train, train_labels)\n",
    "\n",
    "  y_pred = model.predict(X_test)\n",
    "  accuracy = accuracy_score(test_labels, y_pred)\n",
    "  report = classification_report(test_labels, y_pred)\n",
    "\n",
    "  return accuracy, report\n",
    "\n",
    "accuracy, report = train_and_predict_model(\n",
    "  train_features, test_features, train_labels, test_labels\n",
    ")\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn9JEMK_Y6-x"
   },
   "source": [
    "## Part 1. (2c) Evaluate your logistic regression model's performance.<br>\n",
    "**Answer:** My logistic regression model has an accuracy of ~73.5% with similar performance for predicting positive and negative sentiment in tweets. My model has better precision (fewer false positives) for positive sentiment tweets and better recall (fewer false negatives) for negative sentiment tweets. These results show that even a simple logistic regression model can predict sentiment in tweets with decent accuracy when trained with n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5toPn0azZEZI"
   },
   "source": [
    "## Part 1. (2d) Plotting model accuracy vs n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWncpeWF-EwH",
    "outputId": "21c3b06a-1b48-4c13-8725-ed56e7dc887d"
   },
   "outputs": [],
   "source": [
    "# compare cumulative vs stand alone ngram accuracies\n",
    "def get_accuracy_scores(train_texts, test_tests, n_range, cumulative):\n",
    "  accuracy_scores = []\n",
    "  for i in n_range:\n",
    "    r = range(i+1) if cumulative else range(i, i+1)\n",
    "    train_features = [get_text_ngrams(text, r) for text in train_texts]\n",
    "    test_features = [get_text_ngrams(text, r) for text in test_texts]\n",
    "\n",
    "    accuracy, report = train_and_predict_model(\n",
    "      train_features, test_features, train_labels, test_labels\n",
    "    )\n",
    "    print(\n",
    "      f\"{'CUMULATIVE' if cumulative else 'SINGLE'}, n={i}, acc={accuracy:.4f}\"\n",
    "    )\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "  return accuracy_scores\n",
    "\n",
    "n_range = range(1, 10)\n",
    "cumulative_accs = get_accuracy_scores(train_texts, test_texts, n_range, True)\n",
    "single_accs = get_accuracy_scores(train_texts, test_texts, n_range, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "bH-SpCjG-IDH",
    "outputId": "4f28bd44-ebf4-454a-fa5b-908a315a8b07"
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "def create_accuracy_plot(ax, n_range, accuracies, title, cumulative=False):\n",
    "  marker = 's' if cumulative else 'o'\n",
    "  color = '#ff7f0e' if cumulative else '#1f77b4'\n",
    "  ax.plot(\n",
    "    n_range,\n",
    "    accuracies,\n",
    "    marker=marker,\n",
    "    linestyle='-',\n",
    "    color=color,\n",
    "    linewidth=2,\n",
    "    markersize=8\n",
    "  )\n",
    "  ax.set_xlabel('Max N-gram Size (n)' if cumulative else 'N-gram Size (n)')\n",
    "  ax.set_ylabel('Accuracy')\n",
    "  ax.set_title(title, fontsize=14)\n",
    "  ax.grid(True, linestyle='--', alpha=0.7)\n",
    "  ax.set_xticks(n_range)\n",
    "\n",
    "  for i, acc in enumerate(accuracies):\n",
    "    ax.text(n_range[i], acc + 0.001, f'{acc:.4f}', ha='center', fontsize=9)\n",
    "\n",
    "def plot_ngram_comparison(n_range, single_accs, cumulative_accs):\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "  create_accuracy_plot(\n",
    "    ax1, n_range, single_accs, 'Individual N-gram Size'\n",
    "  )\n",
    "  create_accuracy_plot(\n",
    "    ax2, n_range, cumulative_accs, 'Cumulative N-gram Size', True\n",
    "  )\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "plot_ngram_comparison(n_range, single_accs, cumulative_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtbEqQbkKG2K"
   },
   "source": [
    "## Part 1. (2d) How does the value of n in n-gram affect the model's performance?\n",
    "Explain your answer. You can draw a plot with n-gram and the model's performance.<br>\n",
    "**Answer:** The size of n and wether or not the n-grams are cumulative both have an impact on the model's accuracy.\n",
    "\n",
    "In terms of individual contributions to the model's accuracy, the unigrams have the most impact and provide the highest accuracy. This makes sense because individual words provide the majority of the sentiment in sentences. For cumulative n-grams, a maximum size of 3 provides the highest accuracy. After this point increasing the size of n leads to decreasing accuracy. Including up to trigrams provide the best accuracy because sentiment is often conveyed in 1-3 words in the english language. For example, two words are a requirement to convey the sentiment when saying something is 'not good'.\n",
    "\n",
    "Overall unigrams, bigrams, and trigrams make up most of the contributions to the model's accuracy. Any additional increases to n lead to diminishing accuracy because a majority of the sentiment is captured in just 1-3 word n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7eEbiFGbty_"
   },
   "source": [
    "# Part 2. RNN and Machine Translation\n",
    "You will be training a Seq2seq model using RNN. Your input will be a text and the output will be a summary of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikKj3_nMeLWy",
    "outputId": "e9bbc9f2-ae2c-4e15-8f63-59db13c7199f"
   },
   "outputs": [],
   "source": [
    "# imports for RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import (\n",
    "  corpus_bleu, sentence_bleu, SmoothingFunction\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "seed = 2025\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A4GbQYfb5mU"
   },
   "source": [
    "## Part 2. (1) Load the California State bill subset of the BillSum dataset from HuggingFace.\n",
    "Do the following:\n",
    "- Load the test split as your entire dataset for this task.\n",
    "- Split the dataset into a train and test set with the train test split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_TcwVhwWka0",
    "outputId": "3e32ce71-9bc7-4818-adaf-84b98ab4e0d8"
   },
   "outputs": [],
   "source": [
    "# download and load the data\n",
    "# https://huggingface.co/docs/transformers/main/en/tasks/summarization\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "billsum = load_dataset(\"billsum\", split=\"ca_test\").train_test_split(\n",
    "  test_size=0.2, seed=seed\n",
    ")\n",
    "billsum_train, billsum_test = billsum[\"train\"], billsum[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hFnAwP3KFj0"
   },
   "source": [
    "## Part 2. (2) Use the number of neurons, dropout, and your selection of RNN architecture.\n",
    "NOTE: Report BLEU as the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Imce836yOulb"
   },
   "outputs": [],
   "source": [
    "# custom dataset for our seq2seq model\n",
    "class Seq2SeqDataset(Dataset):\n",
    "  def __init__(self, data, combination):\n",
    "    data = data.map(self.preprocess, combination)\n",
    "    self.input_ids = data[\"input_ids\"]\n",
    "    self.labels = data[\"labels\"]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return {\n",
    "      \"input_ids\": torch.tensor(self.input_ids[index], dtype=torch.long),\n",
    "      \"labels\": torch.tensor(self.labels[index], dtype=torch.long)\n",
    "    }\n",
    "\n",
    "  def preprocess(\n",
    "    self, example, input_max_length=1024, summary_max_length=128\n",
    "  ):\n",
    "    tokenized_input = tokenizer.encode_plus(\n",
    "      \"summarize the legal document: \" + example[\"text\"],\n",
    "      max_length=input_max_length,\n",
    "      truncation=True,\n",
    "      padding=\"max_length\"\n",
    "    )\n",
    "    tokenized_summary = tokenizer.encode_plus(\n",
    "      example[\"summary\"],\n",
    "      max_length=summary_max_length,\n",
    "      truncation=True,\n",
    "      padding=\"max_length\"\n",
    "    )\n",
    "    return {\n",
    "      \"input_ids\": tokenized_input[\"input_ids\"],\n",
    "      \"labels\": tokenized_summary[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TqXDTwjOzV6"
   },
   "outputs": [],
   "source": [
    "# seq2seq model w/ LSTM encoder and decoder\n",
    "class Seq2SeqModel(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.1):\n",
    "    super(Seq2SeqModel, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "    self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "    self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, source, target):\n",
    "    source_embedding = self.dropout(self.embedding(source))\n",
    "    _, (h, c) = self.encoder(source_embedding)\n",
    "\n",
    "    target_embedding = self.dropout(self.embedding(target))\n",
    "    decoder_output, _ = self.decoder(target_embedding, (h, c))\n",
    "    logits = self.output_layer(decoder_output)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adB7yjyWO-AY"
   },
   "outputs": [],
   "source": [
    "# train model with specified max tokenization lengths\n",
    "def train_model(model, train_loader, optimizer, criterion, device, epochs=3):\n",
    "  model.train()\n",
    "  for i in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "      source = batch[\"input_ids\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      decoder_input = labels[:,:-1]\n",
    "      decoder_target = labels[:,1:]\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      logits = model(source, decoder_input)\n",
    "      logits = logits.reshape(-1, logits.size(-1))\n",
    "      decoder_target = decoder_target.reshape(-1)\n",
    "\n",
    "      loss = criterion(logits, decoder_target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {i+1}/{epochs} - Loss: {epoch_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rge95IdvKFj3"
   },
   "source": [
    "## Part 2. (3) Vary the input seq length by truncating the main text and the summary text.\n",
    "We will determine how these lengths impact the model's performance:\n",
    "- Main Text Lengths: 1024, 2048\n",
    "- Summary Text Lengths: 128, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I43Fn3DiPAgy"
   },
   "outputs": [],
   "source": [
    "# calculate the corpus-level BLEU-4 score\n",
    "def calculate_bleu(model, test_loader, tokenizer, device):\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  references = []\n",
    "\n",
    "  smoothing = SmoothingFunction().method1\n",
    "  with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "      source = batch[\"input_ids\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      decoder_input = labels[:,:-1]\n",
    "\n",
    "      outputs = model(source, decoder_input)\n",
    "      pred_ids = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "      decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "      decoded_refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "      predictions.extend(decoded_preds)\n",
    "      references.extend([[reference] for reference in decoded_refs])\n",
    "\n",
    "  bleu_score = corpus_bleu(\n",
    "    references,\n",
    "    predictions,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=smoothing\n",
    "  )\n",
    "  return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLEcFHztVUiB"
   },
   "outputs": [],
   "source": [
    "# pad a batch of variable length tokens to be uniform length\n",
    "def collate_fn(batch):\n",
    "  input_ids = []\n",
    "  labels = []\n",
    "  for item in batch:\n",
    "    input_ids.append(item[\"input_ids\"].clone().detach())\n",
    "    labels.append(item[\"labels\"].clone().detach())\n",
    "\n",
    "  padded_input_ids = pad_sequence(\n",
    "    input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "  )\n",
    "  padded_labels = pad_sequence(\n",
    "    labels, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "  )\n",
    "  return {\"input_ids\": padded_input_ids, \"labels\": padded_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8280c77cd0e04908aa6f612d80278533",
      "fc5165a930214162a80c0e265b98a1e7",
      "63f0c5f655924763a174db58fd985041",
      "f058993a66e2409aaa66bd268c733cd6",
      "f7101b87030843d7b07b74c6a0807088",
      "e5c35f2e561a4361bb54a338d324fe4d",
      "994543c0b16543a698bb08916381fbfe",
      "3291413d8462415497017e6965b80859",
      "79a48dd706634a1db2e11eac932345c1",
      "fcf0381362b041abbb286c0337e72704",
      "6c782455c47542d39132c7962bca74f7"
     ]
    },
    "id": "eReY7-HpPDE7",
    "outputId": "03a504f9-9328-4674-c9f4-b29b209f9927"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "seq_length_combinations = [\n",
    "  {'input_max_length': 1024, 'summary_max_length': 128},\n",
    "  {'input_max_length': 1024, 'summary_max_length': 256},\n",
    "  {'input_max_length': 2048, 'summary_max_length': 128},\n",
    "  {'input_max_length': 2048, 'summary_max_length': 256},\n",
    "]\n",
    "\n",
    "for seq_length_combination in seq_length_combinations:\n",
    "  train_dataset = Seq2SeqDataset(billsum_train, seq_length_combination)\n",
    "  test_dataset = Seq2SeqDataset(billsum_test, seq_length_combination)\n",
    "  train_loader = DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn\n",
    "  )\n",
    "  test_loader  = DataLoader(\n",
    "    test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn\n",
    "  )\n",
    "\n",
    "  vocab_size = tokenizer.vocab_size\n",
    "  embed_size = 256\n",
    "  hidden_size = 512\n",
    "\n",
    "  model = Seq2SeqModel(\n",
    "    vocab_size, embed_size, hidden_size, dropout=0.1\n",
    "  ).to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "  train_model(model, train_loader, optimizer, criterion, device, epochs=3)\n",
    "  bleu_score = calculatue_bleu(model, test_loader, tokenizer, device)\n",
    "  print(seq_length_combination)\n",
    "  print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GVX0noXKFj3"
   },
   "source": [
    "## Part 2. (3) How does the sequence length impact the model’s performance?\n",
    "**Answer:** The largest impact on the BLEU score was due to increasing the summary length from 128 to 256."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
