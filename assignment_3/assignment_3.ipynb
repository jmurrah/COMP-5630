{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEfEJVmSXS_h"
   },
   "source": [
    "# PREREQUISITE: Installation Steps\n",
    "add steps here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlReYxVjagE9"
   },
   "outputs": [],
   "source": [
    "SKIP_INSTALL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emr_zPDbDzTV"
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "  if globals().get('SKIP_INSTALL', False):\n",
    "    print(\"Skipping installation cell.\")\n",
    "  else:\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yj6sNu9vcAt6"
   },
   "outputs": [],
   "source": [
    "# SKIP_INSTALL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9A3z2JkJ2R7",
    "outputId": "80b2f013-dbdd-431f-bad8-39f4fec7b1f3"
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "import contextlib\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "\n",
    "def capture_output():\n",
    "  return contextlib.redirect_stdout(io.StringIO())\n",
    "\n",
    "print(\"Downloads take ~5 minutes but are required for version compatiability!\")\n",
    "time.sleep(1)\n",
    "\n",
    "with capture_output():\n",
    "  package_versions = {\n",
    "    \"numpy\": \"1.26.4\",\n",
    "    \"torch\": \"2.2.2\",\n",
    "    \"transformers\": \"4.50.3\",\n",
    "    \"datasets\": \"2.18.0\",\n",
    "    \"scipy\": \"1.12.0\",\n",
    "    \"pandas\": \"2.1.4\",\n",
    "    \"scikit-learn\": \"1.4.0\",\n",
    "    \"gensim\": \"4.3.2\",\n",
    "    \"evaluate\": \"0.4.1\"\n",
    "  }\n",
    "\n",
    "  # get cuda version\n",
    "  cuda_version_str = \"N/A\"\n",
    "  cuda_major_minor = None\n",
    "  nvcc_output = subprocess.check_output(['nvcc', '--version']).decode('utf-8')\n",
    "  match_obj = re.search(r'release (\\d+\\.\\d+)', nvcc_output)\n",
    "  if match_obj:\n",
    "    cuda_version_str = match_obj.group(1)\n",
    "    cuda_major_minor = \"\".join(cuda_version_str.split('.'))\n",
    "\n",
    "  # pytorch installation\n",
    "  pytorch_version = package_versions[\"torch\"]\n",
    "  pytorch_cuda_suffix = \"cu118\"\n",
    "  if cuda_major_minor and int(cuda_major_minor) >= 121:\n",
    "    pytorch_cuda_suffix = \"cu121\"\n",
    "\n",
    "  torchvision_version = \"0.17.2\"\n",
    "  torchaudio_version = \"2.2.2\"\n",
    "  pytorch_index_url = f\"https://download.pytorch.org/whl/{pytorch_cuda_suffix}\"\n",
    "\n",
    "  pytorch_install_command = (\n",
    "    f\"pip3 install \"\n",
    "    f\"torch=={pytorch_version}+{pytorch_cuda_suffix} \"\n",
    "    f\"torchvision=={torchvision_version}+{pytorch_cuda_suffix} \"\n",
    "    f\"torchaudio=={torchaudio_version}+{pytorch_cuda_suffix} \"\n",
    "    f\"--index-url {pytorch_index_url} --upgrade --no-cache-dir\"\n",
    "  )\n",
    "  get_ipython().system(pytorch_install_command)\n",
    "\n",
    "  # install specified packages\n",
    "  get_ipython().system('pip3 uninstall transformers -y')\n",
    "  packages_to_install = [\n",
    "    f\"{pkg}=={ver}\" for pkg, ver in package_versions.items() if pkg != \"torch\"\n",
    "  ]\n",
    "  other_packages_install_command = (\n",
    "    \"pip3 install --upgrade --no-cache-dir \" + \" \".join(packages_to_install)\n",
    "  )\n",
    "\n",
    "  get_ipython().system(other_packages_install_command)\n",
    "  get_ipython().system(\n",
    "    'pip install \"huggingface_hub[hf_xet]\" --upgrade --no-cache-dir'\n",
    "  )\n",
    "\n",
    "# restart environment\n",
    "print(\"Package installation completed successfully!\")\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crWLttL5M5VI",
    "outputId": "d240b7f1-0063-4585-a9e6-f10f95d7cc3e"
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "import importlib.metadata\n",
    "import sys\n",
    "version = importlib.metadata.version(\"torch\")\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE5wn2vIqw0Z"
   },
   "source": [
    "# Part 1. Word Embeddings and N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lr6cxAY9X18x"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6EywEfqWt0K"
   },
   "source": [
    "## Part 1. (1a-1b) You will examine two-word embeddings. You are given the following words:\n",
    "\n",
    "Dog, Bark, Tree, Bank, River, Money.\n",
    "\n",
    "Do the following:\n",
    "- Use Glove-twitter-50D word2vec and compute nxn matrices using cosine similarities for the given words.\n",
    "- Use FastText and compute nxn matrices using cosine similarities for the given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsoKj7EaDwhF",
    "outputId": "02cfda3a-3a88-4703-cf8b-9ee4b17b53e3"
   },
   "outputs": [],
   "source": [
    "# download glove-twitter-50 model\n",
    "nltk.download(\"twitter_samples\")\n",
    "nltk.download('stopwords')\n",
    "if \"glove\" not in globals():\n",
    "  glove = api.load(\"glove-twitter-50\")\n",
    "\n",
    "print(\"downloads completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9i94n6xsc6eW",
    "outputId": "98208f04-60e1-472b-f364-8e1c9f8d9b9f"
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "# https://www.kaggle.com/code/piyushagni5/sentiment-analysis-on-twitter-dataset-nlp\n",
    "def process_tweet(tweet):\n",
    "  stemmer = PorterStemmer()\n",
    "  stopwords_english = stopwords.words('english')\n",
    "\n",
    "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "  tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "  tokenizer = TweetTokenizer(\n",
    "    preserve_case=False, strip_handles=True, reduce_len=True\n",
    "  )\n",
    "  tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "  tweets_clean = []\n",
    "  for word in tweet_tokens:\n",
    "    if (word not in stopwords_english and word not in string.punctuation):\n",
    "      stem_word = stemmer.stem(word)\n",
    "      tweets_clean.append(stem_word)\n",
    "\n",
    "  return tweets_clean\n",
    "\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "common_texts = [\n",
    "  process_tweet(tweet)\n",
    "  for tweet_list in [negative_tweets, positive_tweets]\n",
    "  for tweet in tweet_list\n",
    "]\n",
    "print(f\"Number of tweets: {len(common_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xrAbDUoNRKI",
    "outputId": "57bac5c5-e525-4f6a-fc0c-9a236ba414a5"
   },
   "outputs": [],
   "source": [
    "# train the Word2Vec and FastText models\n",
    "# NOTE: our GloVe w2v model is already loaded\n",
    "if \"w2v\" not in globals():\n",
    "  w2v = Word2Vec(sentences=common_texts, vector_size=50, window=5, min_count=1)\n",
    "if \"ft\" not in globals():\n",
    "  ft = FastText(\n",
    "    vector_size=50, window=5, min_count=1, sentences=common_texts, epochs=10\n",
    "  )\n",
    "print(\"done training models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O07OA-G4ZNh5"
   },
   "outputs": [],
   "source": [
    "# create similarity matrices of the words for each model\n",
    "def create_similarity_matrix(model, words, glove_model=False):\n",
    "  if glove_model:\n",
    "    common_text_words = set([word for text in common_texts for word in text])\n",
    "    glove_words = [\n",
    "      word.lower() for word in glove.key_to_index\n",
    "      if word.lower() in common_text_words\n",
    "    ]\n",
    "    words_in_model = [w for w in words if w in glove_words]\n",
    "  else:\n",
    "    words_in_model = [w for w in words if w in model.wv]\n",
    "\n",
    "  word_vectors = np.array(\n",
    "    [\n",
    "      model.get_vector(word) if glove_model else model.wv[word]\n",
    "      for word in words_in_model\n",
    "    ]\n",
    "  )\n",
    "  similarity_matrix = cosine_similarity(word_vectors)\n",
    "  return similarity_matrix, words_in_model\n",
    "\n",
    "words = [\"dog\", \"bark\", \"tree\", \"bank\", \"river\", \"money\"]\n",
    "glove_matrix, words_in_glove = create_similarity_matrix(\n",
    "  glove, words, glove_model=True\n",
    ")\n",
    "w2v_matrix, words_in_w2v = create_similarity_matrix(w2v, words)\n",
    "ft_matrix, words_in_ft = create_similarity_matrix(ft, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "VCiS0O2fAB6i",
    "outputId": "a2f04356-e687-49e5-98b0-a57344080f63"
   },
   "outputs": [],
   "source": [
    "# plot the similarity matrices for each model\n",
    "def plot_similarity_matrices(matrices, words_lists, titles):\n",
    "  n = len(matrices)\n",
    "  fig, axes = plt.subplots(1, n, figsize=(n * 6, 5))\n",
    "  for i in range(n):\n",
    "    sns.heatmap(\n",
    "      matrices[i],\n",
    "      annot=True,\n",
    "      fmt=\".2f\",\n",
    "      cmap=\"YlGnBu\",\n",
    "      xticklabels=words_lists[i],\n",
    "      yticklabels=words_lists[i],\n",
    "      ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(titles[i], fontsize=14)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "matrices = [glove_matrix, w2v_matrix, ft_matrix]\n",
    "words_lists = [words_in_glove, words_in_w2v, words_in_ft]\n",
    "titles = [\"GloVe Similarity\", \"Word2Vec Similarity\", \"FastText Similarity\"]\n",
    "plot_similarity_matrices(matrices, words_lists, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8JCOqOqBtMx"
   },
   "source": [
    "## Part 1. (1c) Which embedding captures better semantics? Justify your answer.<br>\n",
    "**Answer:** Based on the similarity scores between the 6 words, the GloVe model is better at capturing the semantics, i.e the meaning, of the words. The FastText model is a close second. Since there is not a universal standard similarity score for these specific word pairs I am using my own understanding of the english language to evaluate.\n",
    "\n",
    "Words with multiple meanings:\n",
    "- Bark: Dog barking or tree bark\n",
    "- Bank: River bank or a bank that manages money\n",
    "\n",
    "Related pairs:\n",
    "- Dog-Bark\n",
    "- Tree-Bark\n",
    "- Money-Bank\n",
    "- River-Bank\n",
    "\n",
    "Model Evaluation:\n",
    "- **GloVe:** The similarity matrix shows clear differentiation between semantically distinct words. For example, the similarity between “bark” and “bank” is extremely low and the similarity between \"money\" and \"bank\" is relatively high.\n",
    "- **Word2Vec:** Shows high similarity scores across most of the pairs and does not differentiate unrelated words. For example, the words \"dog\" and \"tree\" are unrelated but have an extremely high similarity score.\n",
    "- **FastText:** Is able to capture semantic of words but at a more moderate extent than GloVe. This moderate scoring does not differentiate as clearly between related and unrelated pairs as GloVe does.\n",
    "\n",
    "Based only on their 6-word similarity matrices, the GloVe model provides more realistic similarity scores and thus captures the semantic similarities between words better than the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSAX_NSMEwsJ",
    "outputId": "01c646f0-f232-4ff1-b081-af0f8eafa814"
   },
   "outputs": [],
   "source": [
    "benchmark = \"simlex999.txt\"\n",
    "result_glove = glove.evaluate_word_pairs(datapath(benchmark))\n",
    "result_w2v = w2v.wv.evaluate_word_pairs(datapath(benchmark))\n",
    "result_ft = ft.wv.evaluate_word_pairs(datapath(benchmark))\n",
    "\n",
    "print(f\"GloVe Spearman Measurements:\")\n",
    "print(f\" - Coefficient: {result_glove[1][0]:.4f}\")\n",
    "print(f\" - P-Value: {result_glove[1][1]:.4f}\")\n",
    "\n",
    "print(f\"Word2Vec Spearman Measurements:\")\n",
    "print(f\" - Coefficient: {result_w2v[1][0]:.4f}\")\n",
    "print(f\" - P-Value: {result_w2v[1][1]:.4f}\")\n",
    "\n",
    "print(f\"FastText Spearman Measurements:\")\n",
    "print(f\" - Coefficient: {result_ft[1][0]:.4f}\")\n",
    "print(f\" - P-Value: {result_ft[1][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcIbTVPtT7YI"
   },
   "source": [
    "## Part 1. (1c) Continued analysis...<br>\n",
    "**Answer:** I wanted to see which model performed better on a larger set of words so I measured the Spearman Coefficients of each model on the 'simlex999' dataset. In our context, the Spearman Coefficient judges how well our model's predictions match a human's analysis of word pairs. A higher coefficient shows a better match. On the 'simlex999' dataset, the GloVe model achieves the highest Spearman coefficient and is the only model with a statistically significant p-value, indicating that it does the best at capturing overall semantic relationships across a broader range of word pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBEXJI1jYdWZ"
   },
   "source": [
    "## Part 1. (2a-2c) N-grams and Classification\n",
    "\n",
    "Do the following:\n",
    "- Split the data 70% training and 30% testing.\n",
    "- Extract n-grams for n in [1, 4]. unigram, bigram, trigram, 4-grams.\n",
    "- Build a logistic regression model using n-gram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xECkSwbOaNnm"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjHz3_gEZ8_E"
   },
   "outputs": [],
   "source": [
    "# split the data into 70% training and 30% testing\n",
    "seed = 2025\n",
    "random.seed(seed)\n",
    "labels = np.array([0] * len(negative_tweets) + [1] * len(positive_tweets))\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "  common_texts, labels, test_size=0.3, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBHZT0Rra0hM"
   },
   "outputs": [],
   "source": [
    "# extract n-grams for n in [1, 4]. unigram, bigram, trigram, 4grams.\n",
    "def get_text_ngrams(text, n_range):\n",
    "  text_ngrams = []\n",
    "  for i in n_range:\n",
    "    text_string = [\"_\".join(gram) for gram in list(ngrams(text, i))]\n",
    "    text_ngrams.extend(text_string)\n",
    "\n",
    "  return \" \".join(text_ngrams)\n",
    "\n",
    "train_features = [get_text_ngrams(text, range(1, 5)) for text in train_texts]\n",
    "test_features = [get_text_ngrams(text, range(1, 5)) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeUFzjSSx6cG",
    "outputId": "77c3824e-94bd-4313-e6b3-e03b58e017f2"
   },
   "outputs": [],
   "source": [
    "# logistic regression model\n",
    "def train_and_predict_model(\n",
    "  train_features, test_features, train_labels, test_labels\n",
    "):\n",
    "  vectorizer = CountVectorizer()\n",
    "  X_train = vectorizer.fit_transform(train_features)\n",
    "  X_test = vectorizer.transform(test_features)\n",
    "\n",
    "  model = LogisticRegression(max_iter=1000)\n",
    "  model.fit(X_train, train_labels)\n",
    "\n",
    "  y_pred = model.predict(X_test)\n",
    "  accuracy = accuracy_score(test_labels, y_pred)\n",
    "  report = classification_report(test_labels, y_pred)\n",
    "\n",
    "  return accuracy, report\n",
    "\n",
    "accuracy, report = train_and_predict_model(\n",
    "  train_features, test_features, train_labels, test_labels\n",
    ")\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn9JEMK_Y6-x"
   },
   "source": [
    "## Part 1. (2c) Evaluate your logistic regression model's performance.<br>\n",
    "**Answer:** My logistic regression model has an accuracy of ~73.5% with similar performance for predicting positive and negative sentiment in tweets. My model has better precision (fewer false positives) for positive sentiment tweets and better recall (fewer false negatives) for negative sentiment tweets. These results show that even a simple logistic regression model can predict sentiment in tweets with decent accuracy when trained with n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5toPn0azZEZI"
   },
   "source": [
    "## Part 1. (2d) Plotting model accuracy vs n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWncpeWF-EwH",
    "outputId": "06e7ae43-ac6e-4774-c661-6f222791784e"
   },
   "outputs": [],
   "source": [
    "# compare cumulative vs stand alone ngram accuracies\n",
    "def get_accuracy_scores(train_texts, test_tests, n_range, cumulative):\n",
    "  accuracy_scores = []\n",
    "  for i in n_range:\n",
    "    r = range(i+1) if cumulative else range(i, i+1)\n",
    "    train_features = [get_text_ngrams(text, r) for text in train_texts]\n",
    "    test_features = [get_text_ngrams(text, r) for text in test_texts]\n",
    "\n",
    "    accuracy, report = train_and_predict_model(\n",
    "      train_features, test_features, train_labels, test_labels\n",
    "    )\n",
    "    print(\n",
    "      f\"{'CUMULATIVE' if cumulative else 'SINGLE'}, n={i}, acc={accuracy:.4f}\"\n",
    "    )\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "  return accuracy_scores\n",
    "\n",
    "n_range = range(1, 10)\n",
    "cumulative_accs = get_accuracy_scores(train_texts, test_texts, n_range, True)\n",
    "single_accs = get_accuracy_scores(train_texts, test_texts, n_range, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "bH-SpCjG-IDH",
    "outputId": "37db0063-3136-4bd8-e36c-d8280785cd08"
   },
   "outputs": [],
   "source": [
    "def create_accuracy_plot(ax, n_range, accuracies, title, cumulative=False):\n",
    "  marker = 's' if cumulative else 'o'\n",
    "  color = '#ff7f0e' if cumulative else '#1f77b4'\n",
    "  ax.plot(\n",
    "    n_range,\n",
    "    accuracies,\n",
    "    marker=marker,\n",
    "    linestyle='-',\n",
    "    color=color,\n",
    "    linewidth=2,\n",
    "    markersize=8\n",
    "  )\n",
    "  ax.set_xlabel('Max N-gram Size (n)' if cumulative else 'N-gram Size (n)')\n",
    "  ax.set_ylabel('Accuracy')\n",
    "  ax.set_title(title, fontsize=14)\n",
    "  ax.grid(True, linestyle='--', alpha=0.7)\n",
    "  ax.set_xticks(n_range)\n",
    "\n",
    "  for i, acc in enumerate(accuracies):\n",
    "    ax.text(n_range[i], acc + 0.001, f'{acc:.4f}', ha='center', fontsize=9)\n",
    "\n",
    "def plot_ngram_comparison(n_range, single_accs, cumulative_accs):\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "  create_accuracy_plot(\n",
    "    ax1, n_range, single_accs, 'Individual N-gram Size'\n",
    "  )\n",
    "  create_accuracy_plot(\n",
    "    ax2, n_range, cumulative_accs, 'Cumulative N-gram Size', True\n",
    "  )\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "plot_ngram_comparison(n_range, single_accs, cumulative_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtbEqQbkKG2K"
   },
   "source": [
    "## Part 1. (2d) How does the value of n in n-gram affect the model's performance?\n",
    "Explain your answer. You can draw a plot with n-gram and the model's performance.<br>\n",
    "**Answer:** The size of n and wether or not the n-grams are cumulative both have an impact on the model's accuracy.\n",
    "\n",
    "In terms of individual contributions to the model's accuracy, the unigrams have the most impact and provide the highest accuracy. This makes sense because individual words provide the majority of the sentiment in sentences. For cumulative n-grams, a maximum size of 3 provides the highest accuracy. After this point increasing the size of n leads to decreasing accuracy. Including up to trigrams provide the best accuracy because sentiment is often conveyed in 1-3 words in the english language. For example, two words are a requirement to convey the sentiment when saying something is 'not good'.\n",
    "\n",
    "Overall unigrams, bigrams, and trigrams make up most of the contributions to the model's accuracy. Any additional increases to n lead to diminishing accuracy because a majority of the sentiment is captured in just 1-3 word n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7eEbiFGbty_"
   },
   "source": [
    "# Part 2. RNN and Machine Translation\n",
    "You will be training a Seq2seq model using RNN. Your input will be a text and the output will be a summary of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikKj3_nMeLWy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A4GbQYfb5mU"
   },
   "source": [
    "## Part 2. (1) Load the California State bill subset of the BillSum dataset from HuggingFace.\n",
    "Do the following:\n",
    "- Load the test split as your entire dataset for this task.\n",
    "- Split the dataset into a train and test set with the train test split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTuqwmsGWhvh",
    "outputId": "c9eae343-57a7-4257-ad3b-e5fc19ca45fc"
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"downloads completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522,
     "referenced_widgets": [
      "0add8d1471fa4eba8c8e7a993fd13e8d",
      "142f20b42ec1414a84d458c4442a4850",
      "f598cf02ae81454686bf6e3b442791cf",
      "dd8fb28c33ac4da88e887c4cb1c119bb",
      "5fa466bd517740a6af9c29e45719a02f",
      "8bc677a2809049a99a83e0e26923eee2",
      "217e7d3d7e9744ee9f205a8a25444349",
      "76245328cd7c47fba749d30772648057",
      "242c7d775bd949e1bae3c2394de92e35",
      "da1ba273887348ad8076cea463830633",
      "809a2eb0d0b744a091aa0897c95f6d37",
      "e40545a2add544b1b05bc6bddea7f251",
      "1d5958a2a31c4544bdace57cef315459",
      "c910cd507ab246e9bc1415078c42ed90",
      "abb2b75cc2e34a7aaa4d5fc461e43b7d",
      "36c7cccf89ff4d328f1cff35085b3a6f",
      "1f58fbc2fa964204a0f64efde0d32f6f",
      "a03792708709470c944b817bd404f225",
      "349a227842b94cd8ae1e0f1bb13f4ad9",
      "0c749ff791db436c82bb2f8c0dfaaf3a",
      "dbdcb21df70d43a88bfe1e9d8c2cfb9d",
      "e6f5486396f449238a14d0a3613c7572",
      "847d6427cbb84594b622fc76e28236e1",
      "6f3c6140e2e146989366aba0b46183fa",
      "03797b33a7f44d73b70fbedc7f511e2f",
      "75b3be808ea64700b7ab6d4f54c7c6c2",
      "460f1be673604e95b506dce046b0f782",
      "88d6b6f1fd824a9b845d9763726acabc",
      "89d1d0b78eb44787b0e56fb0c48b7cdd",
      "fd3d000a64ae496583c86818481b676b",
      "b9c32aeae92e40d0a7329a1193805b43",
      "f9d36e895d8646859e9f28c73f2ca2df",
      "c3fae9c4dc0f43a8903ad33f560bcca1",
      "030b79e5a4c446bbac901c5636302df8",
      "d550cdf2f3c64976be58e5df3541c85d",
      "d77f3c74ac3c4f5b8f00bc0c6abf9bdb",
      "9028dcbf83d14386a3f11dd3f77007bb",
      "4d49e0598e534b61a88ae2e3a19b188a",
      "4da18ff781ed4a498523ee2096b6f4b5",
      "b29ca286cde843218b13079e604e207e",
      "7532892f7a194ba99f9a872f4641a19a",
      "1664e2d32ae14ae48d5537a7f45bc49c",
      "2c3789bd29584644a18457d02fa849a3",
      "e961e75ae63849c6a5edafb31f7b56ac",
      "bb81e446b75142caa4ffc1cf64601e9f",
      "70b676201046469395a558d044ce554b",
      "375148b02e38402ea16a6d70add07aa2",
      "5229b6f84ca24f7ba97a3bd4268cce7c",
      "b7b6d9c6ef2641a38241ea5a1f47a95b",
      "54b7fa52b9da476b8d70d2e04bbf78d6",
      "3c998a11db6b453ebadda5ef656ce762",
      "0edda9bcf1b5459485a529872eb89fa9",
      "63a47f73ea2c4bba886917d5c48dcd2e",
      "3584690eafab4fca90540f782e94bdfa",
      "951d4bd59e274533905649ab13d39f0e",
      "35f31d1145c24a24bfd97d151267546c",
      "c20a91be66004331bae1e97e92a7ccc0",
      "170c3c27f37f4eefb470d910960a42fd",
      "dd58c28e8ab14f4cbe2757289fba543b",
      "ddf672be868d44abbfd9284935753f48",
      "457b7ea3c9ba4855962830fa2dac2755",
      "ae8f66f52167489b8e2f69d40e07d397",
      "85fdd2c860d44e99b8236e24df41b7a8",
      "a215bdc9616f4bf08531f75cde0b0da2",
      "58a90ddb37b14f14a4d0e9c161e3fedd",
      "46b016ba641d45a79a9c0baa4e11bf2b",
      "f48e253759354c7bb49a16c29e20f7c0",
      "2075bc235f144f3a8ca1ffd9b25117fe",
      "1bbd294920bb479486fbbe75b17dbf42",
      "d3153031c9bf4d4d9c0708b62b2da7af",
      "e33af3ac307e41349af6136ccd25bd36",
      "77273c070e974a3bacc7f41b4964ff75",
      "e006c0ac24364add997941dc043bf71b",
      "3baeecca91d042998f3b6d37783d403a",
      "8bba208788f5494595d1ca49c84d0a4e",
      "658ea1197600433abf6f08691ec3aa17",
      "99630e4e6ab446d991dfa8057c76c2db",
      "d8f02283dfba402db20bcfad99ef9e56",
      "75a0a065697040739cd190c84e1c7aeb",
      "7216dcbfb71f4d0796d6c6be1cf9b391",
      "36104bb99fd94ddeaff79b9ebfa31e90",
      "1a4e3918d7484c21b12872b6716acb14",
      "7ec55b913f1940a09b75a207fa1237a0",
      "7d82a47a5c2e4d7cb5f2a24552a90d9e",
      "d346dd01311e4ffc9e07fcff4371f2b4",
      "82e446ba9b6043b68cddc7dcd82f639d",
      "925987c0b3a54674af8408f0738e9c0a",
      "3497f9dffeb54c8db85c2fda148b631b",
      "8891b771c30c4ef49ffc4ea58da872d9",
      "a821069ae82947d6ada704d96832da22",
      "3ab112c7f3a74458bf49038ca072318f",
      "679041371c8c4ad19d3f2c6127b2f098",
      "6b953a24f99a4832a89d1bed13028c67",
      "666d0ecaac6e464fbc9db5f41ef1f926",
      "f875002ca3224d66ae384783769afaea",
      "347f79794a9641918a516ee468aad013",
      "66e0b1dad5e74a0ebe557c208bf5232d",
      "8d6a049e52f949af901438974fc101da",
      "24c8b5a10fc3451fbb90e20d12fac67a",
      "50ee3a8bad324980b0953be3c8bf8062",
      "9bed6d778f1849158cc30ade95235457",
      "4cc437214a0242338eb8c434c0eefe87",
      "af32d8d1bb034a6ca5743a513a29c2df",
      "8c397c6432bd435cacd5262286729287",
      "156b2c96c2844e47b2f9926ea12d193d",
      "75825f042c8f4e009a8a00a43403b54d",
      "0c84e5c1724549fbb613304218967edb",
      "dcfce630d8a0465b8c9f5beb6417c311",
      "280d1c3440d945c6a1048d68a9fcea0e",
      "95fcfadf5f444781a88f57f6960621f7"
     ]
    },
    "id": "y_TcwVhwWka0",
    "outputId": "f3f6dafa-fbd4-4ea6-d15c-ad15ceea8bc9"
   },
   "outputs": [],
   "source": [
    "# load and format the data\n",
    "# https://huggingface.co/docs/transformers/main/en/tasks/summarization\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "  \"ignore\", message=\"The secret `HF_TOKEN` does not exist in your Colab secrets\"\n",
    ")\n",
    "\n",
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
    "billsum = billsum.train_test_split(test_size=0.2)\n",
    "# checkpoint = \"google-t5/t5-small\"\n",
    "checkpoint = \"facebook/bart-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "def get_preprocess_function(input_max_length=1024, summary_max_length=128):\n",
    "  def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(\n",
    "      inputs, max_length=input_max_length, truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "      text_target=examples[\"summary\"],\n",
    "      max_length=summary_max_length,\n",
    "      truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "  return preprocess_function\n",
    "\n",
    "tokenized_billsum = billsum.map(get_preprocess_function(), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPyBLLpZNPcL"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  predictions, labels = eval_pred\n",
    "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "  tokenized_preds = [\n",
    "    nltk.word_tokenize(pred.lower()) for pred in decoded_preds\n",
    "  ]\n",
    "  tokenized_labels = [\n",
    "    [nltk.word_tokenize(label.lower())] for label in decoded_labels\n",
    "  ]\n",
    "\n",
    "  bleu_score = corpus_bleu(tokenized_labels, tokenized_preds)\n",
    "\n",
    "  # BLEU-1, BLEU-2, BLEU-3, and BLEU-4 weights\n",
    "  weights = [\n",
    "    (1.0, 0.0, 0.0, 0.0),\n",
    "    (0.5, 0.5, 0.0, 0.0),\n",
    "    (0.33, 0.33, 0.33, 0.0),\n",
    "    (0.25, 0.25, 0.25, 0.25)\n",
    "  ]\n",
    "\n",
    "  # BLEU-1, BLEU-2, BLEU-3, and BLEU-4 values\n",
    "  bleu_values = []\n",
    "  for i in range(len(weights)):\n",
    "    bleu_values.append(\n",
    "      corpus_bleu(tokenized_labels, tokenized_preds, weights=weights[i])\n",
    "    )\n",
    "\n",
    "  prediction_lens = [\n",
    "    np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "  ]\n",
    "  avg_gen_len = np.mean(prediction_lens)\n",
    "\n",
    "  result = {\"bleu\": bleu_score, \"gen_len\": avg_gen_len}\n",
    "  for i in range(len(weights)):\n",
    "    result[f\"bleu_{i+1}\"] = bleu_values[i]\n",
    "\n",
    "  return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "79WpptmrajIX",
    "outputId": "dd481809-7c7a-4db8-ee90-7e7048980d1c"
   },
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "print(tokenized_billsum)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "  checkpoint,\n",
    "  low_cpu_mem_usage=True,\n",
    "  torch_dtype=torch.float32,\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_billsum_model\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"none\"],\n",
    "    run_name=None,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=1.0,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    ")\n",
    "\n",
    "# Create trainer with our safe metrics function\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate(resume_from_checkpoint=True)\n",
    "print(\"Final evaluation results:\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
