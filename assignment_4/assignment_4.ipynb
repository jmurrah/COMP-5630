{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment-4 COMP-5630 Jacob Murrah\n",
        "## README\n",
        "This notebook loads the housing dataset, computes and visualizes Naive Bayes conditional probabilities for both discrete and continuous features, implements a Gaussian Naive Bayes classifier, runs a Decision Tree classifier by varying the max depth to study over-fitting, and reports performance metrics on training and test splits.\n",
        "\n",
        "## Dependencies\n",
        "- **Python 3.x**\n",
        "- **pandas**\n",
        "- **numpy**\n",
        "- **matplotlib**\n",
        "- **sklearn**\n",
        "\n",
        "*Note: If you are running this notebook in Google Colab, all the required packages are pre-installed.*\n",
        "\n",
        "## Instructions\n",
        "1. **Run All Cells:** Please click on \\\"Runtime\\\" > \\\"Run all\\\" to execute the entire notebook sequentially.\n",
        "2. **Review the Outputs:** The notebook is organized into several sections. Ensure that all cells run without errors.\n"
      ],
      "metadata": {
        "id": "vGkFVFRwpy6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from math import sqrt, pi, exp, log"
      ],
      "metadata": {
        "id": "QdvYKw33qaro"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Naive Bayes\n",
        "See the attached housing data (Asssignment4_Data.xlsx). Each tab in the Excel file contains training and test splits. Your goal is to construct a Naïve Bayes classifier for this data."
      ],
      "metadata": {
        "id": "an9gWcrYrCKM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ydK-9skwpQQI"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_excel(\"Asssignment4_Data.xlsx\", sheet_name=\"Train\")\n",
        "test_data = pd.read_excel(\"Asssignment4_Data.xlsx\", sheet_name=\"Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. (1) Compute and show the conditional probability distribution for each feature.<br>\n",
        "Note: You are expected to do this part of the question by hand.<br>\n",
        "**Answer:**<br>\n",
        "Using Naive Bayes we will predict the price of a house given it's other features. The first step to creating a conditional probability is seperating the housing prices into groups. Since housing prices are continuous I will need to use thresholds to split the housing prices into groups. I choose to split the data into two equal groups (Low and High) with the seperator being the median.<br><br>\n",
        "Median House Price = (5.6039 + 5.8282) / 2 = **5.7161**<br>\n",
        "Low Price House Ids: 1, 2, 3, 4, 5, 6, 8, 12, 15, 16<br>\n",
        "High Price House Ids: 7, 9, 10, 11, 13, 14, 17, 18, 19, 20<br>\n",
        "\n",
        "| Price Category | Prior Probability |\n",
        "| -------------- | ----------------- |\n",
        "| Low Price      | 10/20 = 0.5       |\n",
        "| High Price     | 10/20 = 0.5       |\n",
        "\n",
        "Now I will calculate the conditional probabilities for each feature. For discrete features I will use tables and for continuous features I will use assume normality and calculate the mean and variance. Note that I will be using Add-One (Laplace) Smoothing for ALL of my calculations.\n",
        "$$P(x_i | y) = \\frac{count(x_i, y) + 1}{count(y) + |V|}$$\n",
        "$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n",
        "$$s^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\bigl(x_i - \\bar{x}\\bigr)^2$$\n",
        "$$f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\,\\sigma^2}}\\exp\\!\\Bigl(-\\frac{(x - \\mu)^2}{2\\,\\sigma^2}\\Bigr)$$\n",
        "\n",
        "**Bathrooms Feature (Discrete)**:\n",
        "\n",
        "| # Bathrooms (LOW PRICE) | Count | P(# Bath = X \\| LOW)  |\n",
        "|-------------------------|-------|-----------------------|\n",
        "| 1                       | 10    | (10+1)/(10+3) ≈ 0.846 |\n",
        "| 1.5                     | 0     | (0+1)/(10+3) ≈ 0.077  |\n",
        "| 2.5                     | 0     | (0+1)/(10+3) ≈ 0.077  |\n",
        "\n",
        "| # Bathrooms (HIGH PRICE) | Count | P(# Bath = X \\| HIGH) |\n",
        "|--------------------------|-------|-----------------------|\n",
        "| 1                        | 5     | (5+1)/(10+3) ≈ 0.462  |\n",
        "| 1.5                      | 3     | (3+1)/(10+3) ≈ 0.308  |\n",
        "| 2.5                      | 2     | (2+1)/(10+3) ≈ 0.231  |\n",
        "\n",
        "<br>**Garages Feature (Discrete)**:\n",
        "\n",
        "| # Garages (LOW PRICE) | Count | P(# Garages = X \\| LOW) |\n",
        "|-----------------------|-------|-------------------------|\n",
        "| 0                     | 3     | (3+1)/(10+4) ≈ 0.286    |\n",
        "| 1                     | 6     | (6+1)/(10+4) ≈ 0.500    |\n",
        "| 1.5                   | 0     | (0+1)/(10+4) ≈ 0.071    |\n",
        "| 2                     | 1     | (1+1)/(10+4) ≈ 0.143    |\n",
        "\n",
        "| # Garages (HIGH PRICE) | Count | P(# Garages = X \\| HIGH) |\n",
        "|------------------------|-------|--------------------------|\n",
        "| 0                      | 0     | (0+1)/(10+4) ≈ 0.071     |\n",
        "| 1                      | 3     | (3+1)/(10+4) ≈ 0.286     |\n",
        "| 1.5                    | 2     | (2+1)/(10+4) ≈ 0.214     |\n",
        "| 2                      | 5     | (5+1)/(10+4) ≈ 0.429     |\n",
        "\n",
        "<br>**Rooms Feature (Discrete)**:\n",
        "\n",
        "| # Rooms (LOW PRICE)     | Count | P(# Rooms = X \\| LOW)  |\n",
        "|-------------------------|-------|------------------------|\n",
        "| 5                       | 1     | (1+1)/(10+6) ≈ 0.125   |\n",
        "| 6                       | 7     | (7+1)/(10+6) ≈ 0.500   |\n",
        "| 7                       | 2     | (2+1)/(10+6) ≈ 0.1875  |\n",
        "| 8                       | 0     | (0+1)/(10+6) ≈ 0.0625  |\n",
        "| 9                       | 0     | (0+1)/(10+6) ≈ 0.0625  |\n",
        "| 10                      | 0     | (0+1)/(10+6) ≈ 0.0625  |\n",
        "\n",
        "| # Rooms (HIGH PRICE)    | Count | P(# Rooms = X \\| HIGH) |\n",
        "|-------------------------|-------|------------------------|\n",
        "| 5                       | 1     | (1+1)/(10+6) ≈ 0.125   |\n",
        "| 6                       | 3     | (3+1)/(10+6) ≈ 0.250   |\n",
        "| 7                       | 3     | (3+1)/(10+6) ≈ 0.250   |\n",
        "| 8                       | 1     | (1+1)/(10+6) ≈ 0.125   |\n",
        "| 9                       | 1     | (1+1)/(10+6) ≈ 0.125   |\n",
        "| 10                      | 1     | (1+1)/(10+6) ≈ 0.125   |\n",
        "\n",
        "<br>**Bedrooms Feature (Discrete)**:\n",
        "\n",
        "| # Bedrooms (LOW PRICE)  | Count | P(# Bedrooms = X \\| LOW) |\n",
        "|-------------------------|-------|--------------------------|\n",
        "| 2                       | 1     | (1+1)/(10+4) ≈ 0.143     |\n",
        "| 3                       | 7     | (7+1)/(10+4) ≈ 0.571     |\n",
        "| 4                       | 2     | (2+1)/(10+4) ≈ 0.214     |\n",
        "| 5                       | 0     | (0+1)/(10+4) ≈ 0.0714    |\n",
        "\n",
        "| # Bedrooms (HIGH PRICE) | Count | P(# Bedrooms = X \\| HIGH) |\n",
        "|-------------------------|-------|---------------------------|\n",
        "| 2                       | 1     | (1+1)/(10+4) ≈ 0.143      |\n",
        "| 3                       | 6     | (6+1)/(10+4) ≈ 0.500      |\n",
        "| 4                       | 1     | (1+1)/(10+4) ≈ 0.143      |\n",
        "| 5                       | 2     | (2+1)/(10+4) ≈ 0.214      |\n",
        "\n",
        "<br>**Construction Type Feature (Discrete)**:\n",
        "\n",
        "| Construction Type (LOW PRICE) | Count | P(Construction Type = X \\| LOW)  |\n",
        "|-------------------------|-------|-----------------------------|\n",
        "| Apartment               | 4     | (4+1)/(10+3) ≈ 0.385        |\n",
        "| Condo                   | 2     | (2+1)/(10+3) ≈ 0.231        |\n",
        "| House                   | 4     | (4+1)/(10+3) ≈ 0.385        |\n",
        "\n",
        "| Construction Type (HIGH PRICE) | Count | P(Construction Type = X \\| HIGH)  |\n",
        "|-------------------------|-------|-----------------------------|\n",
        "| Apartment               | 3     | (3+1)/(10+3) ≈ 0.308 |\n",
        "| Condo                   | 4     | (4+1)/(10+3) ≈ 0.385 |\n",
        "| House                   | 3     | (3+1)/(10+3) ≈ 0.308 |"
      ],
      "metadata": {
        "id": "t_yp_5iprz9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>**Land Area Feature (Continuous)**:\n",
        "\n",
        "| Price Type | Sample Mean ($\\bar{x}$)  | Sample Variance ($s^2$) |\n",
        "|------------|------------------|---------------------------|\n",
        "| Low Price  | $\\frac{51.2663}{10}\\approx 5.1267$ | $\\frac{60.1283}{10}\\approx 6.0128$ |\n",
        "| High Price | $\\frac{74.025}{10}\\approx 7.4025$  | $\\frac{53.2068}{10}\\approx 5.3207$ |\n",
        "\n",
        "| House Index | 1 | 2 | 3 | 4 | 5 | 6 | 8 | 12 | 15 | 16 |\n",
        "|-------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
        "| Low Price   | 0.129566 | 0.131648 | 0.082734 | 0.147742 | 0.156703 | 0.156703 | 0.032684 | 0.162435 | 0.162477 | 0.032684 |\n",
        "\n",
        "| House Index | 7 | 9 | 10 | 11 | 13 | 14 | 17 | 18 | 19 | 20 |\n",
        "|-------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
        "| High Price  | 0.137898 | 0.100772 | 0.011193 | 0.158388 | 0.123964 | 0.164357 | 0.107363 | 0.168928 | 0.171491 | 0.170403 |\n",
        "\n",
        "<br>**Living Area Feature (Continuous)**:\n",
        "\n",
        "| Price Type | Sample Mean ($\\bar{x}$)  | Sample Variance ($s^2$) |\n",
        "|------------|------------------|---------------------------|\n",
        "| Low Price  | $\\frac{12.588}{10}\\approx 1.2588$ | $\\frac{0.5406}{10}\\approx 0.0541$ |\n",
        "| High Price | $\\frac{17.009}{10}\\approx 1.7009$  | $\\frac{6.8197}{10}\\approx 0.6820$ |\n",
        "\n",
        "| House Index | 1 | 2 | 3 | 4 | 5 | 6 | 8 | 12 | 15 | 16 |\n",
        "|-------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
        "| Low Price   | 0.914756 | 1.001839 | 1.607403 | 1.703839 | 1.439109 | 0.870899 | 0.997374 | 0.774924 | 1.012563 | 0.997374 |\n",
        "\n",
        "| House Index | 7 | 9 | 10 | 11 | 13 | 14 | 17 | 18 | 19 | 20 |\n",
        "|-------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
        "| High Price  | 0.413410 | 0.055342 | 0.140175 | 0.409173 | 0.328280 | 0.377525 | 0.482597 | 0.467290 | 0.447103 | 0.468994 |\n",
        "\n",
        "<br>**Age of Home Feature (Continuous)**:\n",
        "\n",
        "| Price Type | Sample Mean ($\\bar{x}$)  | Sample Variance ($s^2$) |\n",
        "|------------|------------------|---------------------------|\n",
        "| Low Price  | $\\frac{436}{10}\\approx 43.6$ | $\\frac{1198.2222}{10}\\approx 119.8222$ |\n",
        "| High Price | $\\frac{313}{10}\\approx 31.3$  | $\\frac{1682.3333}{10}\\approx 168.2333$ |\n",
        "\n",
        "| House Index | 1 | 2 | 3 | 4 | 5 | 6 | 8 | 12 | 15 | 16 |\n",
        "|-------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
        "| Low Price   | 0.036058 | 0.008873 | 0.034527 | 0.023208 | 0.036058 | 0.019186 | 0.020787 | 0.016844 | 0.035580 | 0.020787 |\n",
        "\n",
        "| House Index | 7 | 9 | 10 | 11 | 13 | 14 | 17 | 18 | 19 | 20 |\n",
        "|-------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
        "| High Price  | 0.009706 | 0.021886 | 0.012637 | 0.030713 | 0.030604 | 0.030713 | 0.010879 | 0.023786 | 0.016750 | 0.025063 |\n"
      ],
      "metadata": {
        "id": "M7WgfeL69o7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. (1) CONTINUED.\n",
        "Explain how you got the probability distribution for at least two features in detail. Explain How I got the above values.<br>\n",
        "**Answer:**<br>\n",
        "I will explain how I got the probability distribution for the # Bathrooms (discrete) and the Land Area (continuous). Note that all of the values are calculated using the equations and processes I describe below.<br><br>\n",
        "**# Bathrooms (discrete)**: The first step in creating the distribution is listing ALL of the values present in the dataset. Next we count the number of times each value appears in each class. I then calculate the probability for each discrete value using the Add-One (Laplace) smoothing for a specific class. Note that the V value in the Add-One smoothing equation represents the number of unique bathroom values. The other counts are self explainatory. We repeat this for each of our classes, in our case our classes are low and high. Note that this process is the exact same for each discrete feature in our dataset.<br>\n",
        "Example: I will calculate the probability of the high price class having 1.5 bathrooms.<br>\n",
        "$P(1.5 | HIGH) = \\frac{3 + 1}{10 + 3} ≈ 0.308$\n",
        "<br><br>\n",
        "**Land Area (continuous):** For continuous features we must know the distribution of the data. It is commonly known that the continuous features provided follow a Normal Distribution so I ASSUME NORMALITY. In order to make predictions we will plug in a feature's mean and variance into the pdf of the normal distribution. This requires us to calculate the mean and variance for the continuous feature. The equations to do so are provided above and are commonly known. One we have the normal pdf, for every continuous datapoint we will plug it into the pdf to obtain an output. Note that I do this in my COD<br>\n",
        "Example: I will calculate the mean and variance of a LOW PRICE home for the land area feature and predict using the normal equation. Let's use the land area datapoint of 9.52 as an example.\n",
        "$\\bar{x} ≈ \\frac{1}{10} \\sum_{i=1}^{n} x_i ≈ \\frac{51.2663}{10} ≈ 5.1267$<br>\n",
        "$s^2 ≈ \\frac{1}{10 - 1} \\sum_{i=1}^{n} \\bigl(x_i - {5.1267}\\bigr)^2 ≈ \\frac{60.1283}{10 - 1} ≈ 6.0128$<br>\n",
        "$f(9.52 \\mid 5.1267, 6.0128) = \\frac{1}{\\sqrt{2\\pi\\,6.0128}}\\exp\\!\\Bigl(-\\frac{(9.52 - 5.1267)^2}{2 \\cdot 6.0128}\\Bigr) ≈ 0.03268$"
      ],
      "metadata": {
        "id": "NZIiUYuvACO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. (2) Classify the Test Data using your conditional probability distributions and the MAP rule."
      ],
      "metadata": {
        "id": "NJWQvSXvAi5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 5.7161"
      ],
      "metadata": {
        "id": "S9otEuNKmXf_"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "  def __init__(self):\n",
        "    self.probabilities = self.get_probabilities()\n",
        "    self.all_features = self.get_column_key_mapping()\n",
        "\n",
        "  def get_column_key_mapping(self):\n",
        "    return {\n",
        "      \"discrete\": {\n",
        "        \"bathrooms\": \"Bathrooms\",\n",
        "        \"garages\": \"# Garages\",\n",
        "        \"rooms\": \"# Rooms\",\n",
        "        \"bedrooms\": \"# Bedrooms\",\n",
        "        \"construction\": \"Construction type\"\n",
        "      },\n",
        "      \"continuous\": {\n",
        "        \"land_area\": \"Land Area\",\n",
        "        \"living_area\": \"Living area\",\n",
        "        \"home_age\": \"Age of home\"\n",
        "      }\n",
        "    }\n",
        "\n",
        "  def get_probabilities(self):\n",
        "    # prior probability\n",
        "    prob_prior = {\n",
        "      \"low\": 0.5,\n",
        "      \"high\": 0.5\n",
        "    }\n",
        "\n",
        "    # discrete features\n",
        "    prob_bathrooms = {\n",
        "      1: {\"low\": 0.846, \"high\": 0.462},\n",
        "      1.5: {\"low\": 0.077, \"high\": 0.308},\n",
        "      2.5: {\"low\": 0.077, \"high\": 0.231},\n",
        "    }\n",
        "    prob_garages = {\n",
        "      0: {\"low\": 0.286, \"high\": 0.071},\n",
        "      1: {\"low\": 0.500, \"high\": 0.286},\n",
        "      1.5: {\"low\": 0.071, \"high\": 0.214},\n",
        "      2: {\"low\": 0.143, \"high\": 0.429}\n",
        "    }\n",
        "    prob_rooms = {\n",
        "      5: {\"low\": 0.125, \"high\": 0.125},\n",
        "      6: {\"low\": 0.500, \"high\": 0.250},\n",
        "      7: {\"low\": 0.1875, \"high\": 0.250},\n",
        "      8: {\"low\": 0.0625, \"high\": 0.125},\n",
        "      9: {\"low\": 0.0625, \"high\": 0.125},\n",
        "      10: {\"low\": 0.0625, \"high\": 0.125}\n",
        "    }\n",
        "    prob_bedrooms = {\n",
        "      2: {\"low\": 0.143, \"high\": 0.143},\n",
        "      3: {\"low\": 0.571, \"high\": 0.500},\n",
        "      4: {\"low\": 0.214, \"high\": 0.143},\n",
        "      5: {\"low\": 0.0714, \"high\": 0.214}\n",
        "    }\n",
        "    prob_construction = {\n",
        "      \"Apartment\": {\"low\": 0.385, \"high\": 0.308},\n",
        "      \"Condo\": {\"low\": 0.231, \"high\": 0.385},\n",
        "      \"House\": {\"low\": 0.385, \"high\": 0.308}\n",
        "    }\n",
        "\n",
        "    # continuous features\n",
        "    prob_land_area = {\n",
        "      \"low\": {\"mean\": 5.1267, \"var\": 6.0128},\n",
        "      \"high\": {\"mean\": 7.4025, \"var\": 5.3207}\n",
        "    }\n",
        "    prob_living_area = {\n",
        "      \"low\": {\"mean\": 1.2588, \"var\": 0.0541},\n",
        "      \"high\": {\"mean\": 1.7009, \"var\": 0.682}\n",
        "    }\n",
        "    prob_home_age = {\n",
        "      \"low\": {\"mean\": 43.6, \"var\": 119.8222},\n",
        "      \"high\": {\"mean\": 31.3, \"var\": 168.2333}\n",
        "    }\n",
        "\n",
        "    return {\n",
        "      \"prior\": prob_prior,\n",
        "      \"bathrooms\": prob_bathrooms,\n",
        "      \"garages\": prob_garages,\n",
        "      \"rooms\": prob_rooms,\n",
        "      \"bedrooms\": prob_bedrooms,\n",
        "      \"construction\": prob_construction,\n",
        "      \"land_area\": prob_land_area,\n",
        "      \"living_area\": prob_living_area,\n",
        "      \"home_age\": prob_home_age\n",
        "    }\n",
        "\n",
        "  def calculate_normal_pdf(self, x, mean, var):\n",
        "    return (1 / sqrt(2 * pi * var)) * exp(-((x - mean) ** 2) / (2 * var))\n",
        "\n",
        "  def process_feature(self, data_type, log_probs, prob_key, feature):\n",
        "    if data_type == \"discrete\":\n",
        "      log_probs[\"low\"] += log(self.probabilities[prob_key][feature][\"low\"])\n",
        "      log_probs[\"high\"] += log(self.probabilities[prob_key][feature][\"high\"])\n",
        "    else:\n",
        "      pdf_low = self.calculate_normal_pdf(\n",
        "        feature,\n",
        "        self.probabilities[prob_key][\"low\"][\"mean\"],\n",
        "        self.probabilities[prob_key][\"low\"][\"var\"]\n",
        "      )\n",
        "      pdf_high = self.calculate_normal_pdf(\n",
        "        feature,\n",
        "        self.probabilities[prob_key][\"high\"][\"mean\"],\n",
        "        self.probabilities[prob_key][\"high\"][\"var\"]\n",
        "      )\n",
        "      log_probs[\"low\"] += log(pdf_low)\n",
        "      log_probs[\"high\"] += log(pdf_high)\n",
        "\n",
        "  def predict(self, house_data):\n",
        "    log_probs = {\n",
        "      \"low\": log(self.probabilities[\"prior\"][\"low\"]),\n",
        "      \"high\": log(self.probabilities[\"prior\"][\"high\"])\n",
        "    }\n",
        "    for data_type, features in self.all_features.items():\n",
        "      for prob_key, column_name in features.items():\n",
        "        self.process_feature(\n",
        "          data_type, log_probs, prob_key, house_data[column_name]\n",
        "        )\n",
        "\n",
        "    # convert log probabilities back to normal\n",
        "    max_log_prob = max(log_probs[\"low\"], log_probs[\"high\"])\n",
        "    probs = {\n",
        "      \"low\": exp(log_probs[\"low\"] - max_log_prob),\n",
        "      \"high\": exp(log_probs[\"high\"] - max_log_prob)\n",
        "    }\n",
        "\n",
        "    # normalize\n",
        "    total_prob = probs[\"low\"] + probs[\"high\"]\n",
        "    probs[\"low\"] /= total_prob\n",
        "    probs[\"high\"] /= total_prob\n",
        "\n",
        "    return {\n",
        "      \"prob_low\": probs[\"low\"],\n",
        "      \"prob_high\": probs[\"high\"],\n",
        "      \"pred\": \"Low Price\" if probs[\"low\"] > probs[\"high\"] else \"High Price\"\n",
        "    }\n",
        "\n",
        "  def predict_batch(self, data, label):\n",
        "    results = []\n",
        "    for _, house_data in data.iterrows():\n",
        "      prediction = nbc.predict(house_data)\n",
        "      expectation = (\n",
        "        \"Low Price\" if house_data[\"Local Price\"] <= THRESHOLD else \"High Price\"\n",
        "      )\n",
        "      result = {\n",
        "        \"House ID\": house_data[\"House ID\"],\n",
        "        \"Price\": house_data[\"Local Price\"],\n",
        "        \"Probability Low\": prediction[\"prob_low\"],\n",
        "        \"Probability High\": prediction[\"prob_high\"],\n",
        "        \"Prediction\": prediction[\"pred\"],\n",
        "        \"Expectation\": expectation,\n",
        "        \"Correct\": int(prediction[\"pred\"] == expectation)\n",
        "      }\n",
        "      results.append(result)\n",
        "      if label == \"Testing Data\":\n",
        "        print(\n",
        "          f\"House {house_data['House ID']}: P(Low)={result['Probability Low']:.4f} P(High)={result['Probability High']:.4f} → Prediction: {result['Prediction']}\"\n",
        "        )\n",
        "\n",
        "    return results\n",
        "\n",
        "  def print_metrics(self, results, label):\n",
        "    print(f\"\\n--- {label} Prediction Results ---\")\n",
        "    total_correct = sum(r[\"Correct\"] for r in results)\n",
        "    print(f\"Total CORRECT Predictions: {total_correct}\")\n",
        "    print(f\"Total INCORRECT Predictions: {len(results) - total_correct}\")\n",
        "    print(f\"{label} Accuracy: {total_correct / len(results)}\")"
      ],
      "metadata": {
        "id": "3QFc-rY9EWCl"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Low Prices <= {THRESHOLD}\\tHigh Prices > {THRESHOLD}\\n\")\n",
        "test_label = \"Testing Data\"\n",
        "train_label = \"Training Data\"\n",
        "\n",
        "nbc = NaiveBayesClassifier()\n",
        "\n",
        "print(\"--- Testing Data House Price Probabilities ---\")\n",
        "test_results = nbc.predict_batch(test_data, test_label)\n",
        "train_results = nbc.predict_batch(train_data, train_label)\n",
        "\n",
        "nbc.print_metrics(test_results, test_label)\n",
        "nbc.print_metrics(train_results, train_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGjcPqAOInGj",
        "outputId": "d81d1c79-aeea-4173-862a-3da0a1c5975f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Low Prices <= 5.7161\tHigh Prices > 5.7161\n",
            "\n",
            "--- Testing Data House Price Probabilities ---\n",
            "House 24: P(Low)=0.6174 P(High)=0.3826 → Prediction: Low Price\n",
            "House 25: P(Low)=0.0186 P(High)=0.9814 → Prediction: High Price\n",
            "House 26: P(Low)=0.0088 P(High)=0.9912 → Prediction: High Price\n",
            "House 27: P(Low)=0.0052 P(High)=0.9948 → Prediction: High Price\n",
            "House 28: P(Low)=0.4666 P(High)=0.5334 → Prediction: High Price\n",
            "\n",
            "--- Testing Data Prediction Results ---\n",
            "Total CORRECT Predictions: 4\n",
            "Total INCORRECT Predictions: 1\n",
            "Testing Data Accuracy: 0.8\n",
            "\n",
            "--- Training Data Prediction Results ---\n",
            "Total CORRECT Predictions: 16\n",
            "Total INCORRECT Predictions: 4\n",
            "Training Data Accuracy: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. Decision Tree\n",
        "Using the same housing data (Asssignment4 Data.xlsx), construct a decision tree\n",
        "classifier. You can use the implementation available on Sci-Kit Learn."
      ],
      "metadata": {
        "id": "J7Xcky02kWau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 5.7161\n",
        "RANDOM_STATE = 2000"
      ],
      "metadata": {
        "id": "6zohKLaQmT32"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "train_data = pd.read_excel('Asssignment4_Data.xlsx', sheet_name='Train')\n",
        "test_data = pd.read_excel('Asssignment4_Data.xlsx', sheet_name='Test')\n",
        "\n",
        "train_data['label'] = (train_data['Local Price'] > THRESHOLD).astype(int)\n",
        "test_data['label'] = (test_data['Local Price'] > THRESHOLD).astype(int)\n",
        "\n",
        "features = [\n",
        "  \"Bathrooms\",\n",
        "  \"# Garages\",\n",
        "  \"# Rooms\",\n",
        "  \"# Bedrooms\",\n",
        "  \"Construction type\",\n",
        "  \"Land Area\",\n",
        "  \"Living area\",\n",
        "  \"Age of home\",\n",
        "]\n",
        "\n",
        "X_train = train_data[features]\n",
        "y_train = train_data['label']\n",
        "\n",
        "X_test  = test_data[features]\n",
        "y_test  = test_data['label']\n",
        "\n",
        "X_train_encoded = pd.get_dummies(X_train, columns=['Construction type'])\n",
        "X_test_encoded  = pd.get_dummies(X_test,  columns=['Construction type'])\n",
        "X_test_encoded = X_test_encoded.reindex(\n",
        "  columns=X_train_encoded.columns, fill_value=0\n",
        ")\n",
        "feature_names = X_train_encoded.columns.tolist()"
      ],
      "metadata": {
        "id": "e5XTcrk1msM9"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. (1) Use the default parameters."
      ],
      "metadata": {
        "id": "EsZmWpGrkktI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtc_default = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "dtc_default.fit(X_train_encoded, y_train)\n",
        "train_accuracy_default = dtc_default.score(X_train_encoded, y_train)\n",
        "test_accuracy_default = dtc_default.score(X_test_encoded, y_test)\n",
        "\n",
        "print(\"Default parameters:\")\n",
        "print(f\"(a) Training Data Accuracy = {train_accuracy_default:.3f}\")\n",
        "print(f\"(b) Testing Data Accuracy = {test_accuracy_default:.3f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFvBHbdrlCPe",
        "outputId": "4fac9333-f1db-42c9-d70e-829ac04d4145"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default parameters:\n",
            "(a) Training Data Accuracy = 1.000\n",
            "(b) Testing Data Accuracy = 0.800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. (2) What is the effect of restricting the maximum depth of the tree? Try different depths and find the best value."
      ],
      "metadata": {
        "id": "e0FuVl36pvgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for depth in range(1, 11):\n",
        "  dtc = DecisionTreeClassifier(max_depth=depth, random_state=RANDOM_STATE)\n",
        "  dtc.fit(X_train_encoded, y_train)\n",
        "\n",
        "  train_acc = dtc.score(X_train_encoded, y_train)\n",
        "  test_acc = dtc.score(X_test_encoded,  y_test)\n",
        "  results.append((depth, train_acc, test_acc))\n",
        "\n",
        "print(\"Accuracy at Varying Depths:\")\n",
        "for depth, train_acc, test_acc in results:\n",
        "  print(f\"depth {depth:2d}: train = {train_acc:.3f}, test = {test_acc:.3f}\")\n",
        "\n",
        "best_depth, best_train, best_test = max(results, key=lambda x: x[2])\n",
        "print(f\"\\nBest Value: max_depth = {best_depth}, train = {best_train:.3f}, test = {best_test:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei_8B_OxG_I-",
        "outputId": "bbc4c80e-00a1-42e8-aa12-b8c828dc11d4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy at Varying Depths:\n",
            "depth  1: train = 0.900, test = 0.800\n",
            "depth  2: train = 1.000, test = 0.800\n",
            "depth  3: train = 1.000, test = 0.800\n",
            "depth  4: train = 1.000, test = 0.800\n",
            "depth  5: train = 1.000, test = 0.800\n",
            "depth  6: train = 1.000, test = 0.800\n",
            "depth  7: train = 1.000, test = 0.800\n",
            "depth  8: train = 1.000, test = 0.800\n",
            "depth  9: train = 1.000, test = 0.800\n",
            "depth 10: train = 1.000, test = 0.800\n",
            "\n",
            "Best Value: max_depth = 1, train = 0.900, test = 0.800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. (3) Why does restricting the depth have such a strong effect on the classifier performance?\n",
        "**Answer:**<br>\n",
        "In general when using decision trees, restricting the depth is great way to prevent overfitting. In the above example, keeping the depth at 1 would produce the same results as having a depth of 10. As a result, we should keep the depth at 1, for this example, since the decision tree with this depth will generalize much better than a tree of depth 10. In practice, pruned/shallow trees act as a great regularizer that captures the the main patterns in the data. Since our test dataset is so small it is difficult to fully appreciate the significance of pruning as our test accuracy remains the same for every depth. However, in larger datasets we typically see that shallow trees generalize better than larger trees."
      ],
      "metadata": {
        "id": "F0UOsDi6qNm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. (4) For test data point, perform inference on decision tree."
      ],
      "metadata": {
        "id": "ybHU2GBRqYrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_datapoint = {\n",
        "  \"Local Price\": 9.0384,\n",
        "  \"Bathrooms\": 1,\n",
        "  \"Land Area\": 7.8,\n",
        "  \"Living Area\": 1.5,\n",
        "  \"# Garages\": 1.5,\n",
        "  \"# Rooms\": 7,\n",
        "  \"# Bedrooms\": 3,\n",
        "  \"Age of home\": 23\n",
        "}\n",
        "df_datapoint = pd.DataFrame(\n",
        "  [test_datapoint], columns=feature_names\n",
        ")\n",
        "\n",
        "probs = dtc_default.predict_proba(df_datapoint)[0]\n",
        "pred = dtc_default.predict(df_datapoint)[0]\n",
        "label = \"Low Price\" if pred == 0 else \"High Price\"\n",
        "\n",
        "print(\"Inference on the given test datapoint:\")\n",
        "print(f\"P(Low) = {probs[0]:.3f}\")\n",
        "print(f\"P(High) = {probs[1]:.3f}\")\n",
        "print(f\"Prediction: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLU7gBlRqgYq",
        "outputId": "e151f752-9cb7-4466-f614-dc23ad0eac95"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference on the given test datapoint:\n",
            "P(Low) = 0.000\n",
            "P(High) = 1.000\n",
            "Prediction: High Price\n"
          ]
        }
      ]
    }
  ]
}